# üöÄ Airflow ML Training Pipeline

## The Big Picture

This project demonstrates a **production-ready machine learning training pipeline** that automatically orchestrates the entire lifecycle of training a model in the cloud. Here's the real-world problem it solves:

**Problem:** Training machine learning models requires significant computational resources (GPUs, memory, CPU), but you don't want to keep expensive cloud servers running 24/7. You also want to ensure your code passes all tests before training, and you want to track your experiments automatically.

**Solution:** This Airflow pipeline automates the entire process:
1. ‚úÖ Waits for your code to pass GitHub CI tests
2. üèóÔ∏è Spins up a temporary EC2 instance in AWS
3. üê≥ Sets up the training environment (Docker, MLflow)
4. üéØ Runs your ML training job remotely
5. üßπ Automatically shuts down the instance (even if training fails!)

This is a **cost-effective, automated, and reliable** way to run expensive ML training jobs without manual intervention.

---

## The Tech Stack

### Core Technologies

- **Apache Airflow** - The workflow orchestration engine. Airflow allows us to define complex workflows as Directed Acyclic Graphs (DAGs), schedule them, monitor them, and handle failures gracefully. Think of it as a "smart task scheduler" that can handle dependencies, retries, and data passing between tasks.

- **Docker & Docker Compose** - Containerization and local development. Docker Compose runs the entire Airflow stack (scheduler, webserver, database, Redis) locally, making it easy to develop and test without complex installations.

- **AWS EC2** - Cloud compute resources. EC2 provides on-demand virtual servers where we run our training. We use it because it's scalable, pay-per-use, and integrates seamlessly with other AWS services.

- **Boto3** - The AWS Python SDK. Boto3 is used to programmatically create, manage, and terminate EC2 instances. It's the bridge between our Python code and AWS services.

- **MLflow** - Machine Learning experiment tracking. MLflow automatically logs your model parameters, metrics, and artifacts. This helps you compare different training runs and reproduce results.

- **GitHub API** - CI/CD integration. We poll GitHub's API to check if your latest code changes have passed all CI tests before starting expensive training.

- **Paramiko** - SSH client library. Paramiko allows us to securely connect to the EC2 instance and execute commands remotely, without needing to manually SSH in.

- **PostgreSQL** - Airflow's metadata database. Stores DAG definitions, task execution history, connections, and variables.

- **Redis** - Message broker for Celery. When using CeleryExecutor (as we do), Redis queues tasks for distributed execution across multiple workers.

- **Ngrok** - Secure tunnel service (optional). Ngrok creates a public HTTPS URL that tunnels to your local Airflow web server. This is useful for accessing your Airflow UI from anywhere on the internet, or for receiving webhooks from external services. The ngrok web interface (for inspecting requests) is available at http://localhost:4040.

---

## Project Architecture

```
Airflow-sample-server/
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ ml_training_pipeline.py    # The main DAG definition - your workflow logic lives here
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ airflow.cfg                 # Airflow configuration file (optional custom settings)
‚îÇ
‚îú‚îÄ‚îÄ logs/                           # Task execution logs (auto-generated by Airflow)
‚îÇ   ‚îî‚îÄ‚îÄ dag_id=github_ec2_ml_training/
‚îÇ       ‚îî‚îÄ‚îÄ run_id=.../             # Each DAG run creates a new log directory
‚îÇ
‚îú‚îÄ‚îÄ plugins/                        # Custom Airflow plugins (currently empty, but ready for extensions)
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yaml             # Defines all Airflow services (scheduler, webserver, DB, etc.)
‚îÇ
‚îú‚îÄ‚îÄ .env                            # Environment variables (AWS credentials, GitHub tokens, etc.)
‚îÇ                                  # ‚ö†Ô∏è Never commit this file! (It's in .gitignore)
‚îÇ
‚îî‚îÄ‚îÄ .gitignore                      # Tells Git which files to ignore (logs, .env, __pycache__)
```

### Key Folders Explained

- **`/dags`** - This is where Airflow looks for your workflow definitions. Any Python file in this directory that defines a `DAG` object will be automatically discovered and loaded into the Airflow UI.

- **`/config`** - Optional Airflow configuration overrides. The `airflow.cfg` file here allows you to customize Airflow settings without modifying the Docker image.

- **`/logs`** - Execution logs for every task run. Airflow automatically creates a directory structure here: `dag_id/run_id/task_id/attempt.log`. This is invaluable for debugging failed tasks.

- **`/plugins`** - For custom Airflow operators, hooks, sensors, or executors. Currently empty, but this is where you'd add reusable components if you build more DAGs.

- **`docker-compose.yaml`** - The orchestration file that defines all services needed to run Airflow:
  - `airflow-scheduler` - Parses DAGs and schedules tasks
  - `airflow-apiserver` - Serves the web UI (port 8080)
  - `airflow-worker` - Executes tasks (using Celery)
  - `postgres` - Metadata database
  - `redis` - Message broker
  - `airflow-init` - One-time initialization service

---

## The Code Walkthrough

### Flow 1: The Complete Training Pipeline (Main Flow)

This is the end-to-end journey from "code pushed to GitHub" to "model trained and instance cleaned up."

#### Step-by-Step Data Flow:

1. **`wait_for_github_ci()`** (Task 1)
   - **What it does:** Polls GitHub's API every 30 seconds to check if the latest CI run on the `main` branch has passed.
   - **Why it matters:** We don't want to waste money training on broken code!
   - **Key code:** Uses `requests.get()` to hit `https://api.github.com/repos/{repo}/actions/runs` with authentication headers.
   - **Output:** Returns `True` when CI passes, or raises an exception if it fails or times out.

2. **`create_ec2_instance`** (Task 2)
   - **What it does:** Uses Airflow's `EC2CreateInstanceOperator` to launch a new EC2 instance in AWS.
   - **Key configuration:**
     - `UserData`: A base64-encoded bash script that runs when the instance boots. This script installs Docker, MLflow, and Python dependencies automatically.
     - `InstanceType`: The size of the instance (e.g., `t3.small`).
     - `SecurityGroupIds`: Firewall rules that allow SSH access.
   - **Output:** Returns a list of instance IDs (XCom) that subsequent tasks can use.

3. **`check_ec2_status()`** (Task 3)
   - **What it does:** Waits for AWS's "status checks" to pass. This ensures the instance is fully booted and the network is ready.
   - **How it works:** Uses Boto3's `waiter` pattern, which polls AWS until the instance is ready.
   - **Why it's needed:** Even though the instance is "running," it might not be ready for SSH yet.
   - **Output:** Returns the validated instance ID.

4. **`get_public_ip()`** (Task 4)
   - **What it does:** Retrieves the public IP address of the EC2 instance.
   - **Why it's needed:** We need the IP to SSH into the instance.
   - **Output:** Returns the public IP as a string.

5. **`run_training_via_ssh()`** (Task 5)
   - **What it does:** 
     - Connects to the EC2 instance via SSH using Paramiko
     - Waits for MLflow to be fully installed (checks with `python3 -c "import mlflow"`)
     - Executes the training command: `mlflow run https://github.com/{repo} --version main --build-image`
   - **Key details:**
     - Uses retry logic (10 attempts) to handle SSH connection delays
     - Sets environment variables (MLflow tracking URI, AWS credentials) before running training
     - Streams output in real-time so you can see training progress in Airflow logs
   - **Output:** Training results are logged to MLflow (not returned to Airflow).

6. **`terminate_ec2_instance`** (Task 6)
   - **What it does:** Shuts down and terminates the EC2 instance to stop incurring costs.
   - **Critical feature:** Uses `TriggerRule.ALL_DONE`, meaning it runs **even if training failed**. This prevents orphaned instances from costing money.
   - **How it gets the instance ID:** Uses XCom to pull the instance ID from Task 2: `{{ task_instance.xcom_pull(task_ids='create_ec2_instance')[0] }}`

#### Visual Flow Diagram:

```mermaid
graph TD
    A[wait_for_github_ci] -->|CI Passed| B[create_ec2_instance]
    B -->|Instance IDs| C[check_ec2_status]
    C -->|Validated Instance ID| D[get_public_ip]
    D -->|Public IP| E[run_training_via_ssh]
    E -->|Training Complete| F[terminate_ec2_instance]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#fff4e1
    style D fill:#fff4e1
    style E fill:#e8f5e9
    style F fill:#ffebee
```

### Flow 2: Data Passing Between Tasks (XCom)

Airflow tasks can pass data to each other using **XCom** (cross-communication). This is how the instance ID flows from Task 2 ‚Üí Task 3 ‚Üí Task 4.

**How it works:**
- **Standard Operators** (like `EC2CreateInstanceOperator`) automatically push their return value to XCom.
- **TaskFlow Tasks** (decorated with `@task`) can receive XCom values as function parameters.
- **Example:** `check_ec2_status(create_ec2.output)` - The `.output` property is an XComArg that Airflow resolves at runtime.

**Key Code Pattern:**
```python
# Task 2 returns instance_ids (automatically pushed to XCom)
create_ec2 = EC2CreateInstanceOperator(...)

# Task 3 receives it as a parameter
@task
def check_ec2_status(instance_ids):  # instance_ids comes from XCom!
    instance_id = instance_ids[0]
    # ... do work ...
    return instance_id  # This is also pushed to XCom

# Task 4 receives Task 3's output
@task
def get_public_ip(instance_id):  # instance_id comes from Task 3's XCom!
    # ... do work ...
    return public_ip
```

### Flow 3: EC2 Instance Lifecycle Management

This flow shows how we ensure the instance is properly set up and cleaned up.

1. **Creation with User Data Script:**
   - When EC2 boots, it runs the `USER_DATA_SCRIPT` (embedded in the DAG).
   - This script installs Docker, MLflow, Python packages, and configures permissions.
   - It creates a marker file (`/tmp/airflow_ready`) when done.

2. **Status Verification:**
   - `check_ec2_status()` ensures AWS's health checks pass.
   - `run_training_via_ssh()` has additional checks: it waits for MLflow to be importable before running training.

3. **Cleanup Guarantee:**
   - `terminate_ec2_instance` uses `TriggerRule.ALL_DONE` to ensure it runs regardless of upstream task success or failure.
   - This prevents cost leaks from failed runs.

---

## Key Learning Moments

### 1. **Task Dependencies and Data Flow** (`ml_training_pipeline.py`, lines 293-311)

**What to study:** Notice how tasks are connected both implicitly (through data dependencies) and explicitly (using the `>>` operator).

```python
# Implicit dependency: public_ip depends on validated_instance_id
public_ip = get_public_ip(validated_instance_id)

# Explicit dependency: ensures the graph renders correctly in UI
github_signal >> create_ec2
create_ec2 >> validated_instance_id >> public_ip >> training_output >> terminate_ec2
```

**Why it's important:** 
- **Implicit dependencies** (function parameters) are the "real" dependencies - Airflow won't run a task until its inputs are ready.
- **Explicit dependencies** (`>>`) help with UI visualization and make the code more readable.

**Learning takeaway:** In Airflow, you can mix both patterns. The data flow (XCom) is what actually enforces execution order, but explicit chaining helps with clarity.

### 2. **Error Handling and Resource Cleanup** (`ml_training_pipeline.py`, lines 282-287)

**What to study:** The `terminate_ec2_instance` task uses `TriggerRule.ALL_DONE` instead of the default `ALL_SUCCESS`.

```python
terminate_ec2 = EC2TerminateInstanceOperator(
    task_id="terminate_ec2_instance",
    instance_ids="{{ task_instance.xcom_pull(task_ids='create_ec2_instance')[0] }}",
    trigger_rule=TriggerRule.ALL_DONE,  # ‚Üê Key difference!
)
```

**Why it's important:**
- **Default behavior:** Tasks only run if all upstream tasks succeed.
- **With `ALL_DONE`:** The task runs regardless of success or failure.
- **Real-world impact:** If training fails, the instance still gets terminated, preventing cost leaks.

**Learning takeaway:** Always think about cleanup tasks when working with cloud resources. Use `TriggerRule.ALL_DONE` for cleanup operations that must run even on failure.

### 3. **Retry Logic and Resilience** (`ml_training_pipeline.py`, lines 204-210, 217-228)

**What to study:** Multiple retry patterns throughout the code:

```python
# Retry SSH connection
for i in range(10):
    try:
        ssh.connect(public_ip, username='ubuntu', pkey=ssh_key, timeout=10)
        break
    except Exception as e:
        print(f"Waiting for SSH... ({e})")
        time.sleep(10)

# Retry MLflow import check
wait_cmd = """
for i in {1..60}; do
    if python3 -c "import mlflow" > /dev/null 2>&1; then
        echo "‚úÖ Python found MLflow!"
        exit 0
    fi
    sleep 5
done
"""
```

**Why it's important:**
- Cloud resources take time to become ready (eventual consistency).
- Network operations can fail transiently.
- User data scripts run asynchronously - the instance might be "running" but not fully configured.

**Learning takeaway:** When working with cloud APIs and remote systems, always implement retry logic with exponential backoff or polling. Don't assume resources are immediately available.

---

## Setup & Run

### Prerequisites

- **Docker Desktop** installed and running
- **AWS Account** with EC2 permissions
- **GitHub Personal Access Token** with `repo` and `actions:read` scopes
- **SSH Key Pair** created in AWS EC2

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd Airflow-sample-server
```

### Step 1a: Create AWS SSH Key Pair

You need an SSH key pair to securely connect to your EC2 instances. Here's how to create one in AWS:

**Option 1: Using AWS Console (Recommended for beginners)**

1. Log in to the [AWS Console](https://console.aws.amazon.com/)
2. Navigate to **EC2** ‚Üí **Key Pairs** (under "Network & Security" in the left sidebar)
3. Click **"Create key pair"**
4. Configure:
   - **Name:** Choose a descriptive name (e.g., `airflow-training-key`)
   - **Key pair type:** Select **RSA**
   - **Private key file format:** Select **.pem** (for Linux/Mac) or **.ppk** (for Windows with PuTTY)
5. Click **"Create key pair"**
6. **‚ö†Ô∏è Important:** The private key file will automatically download. Save it securely - you'll need it for the `SSH_PRIVATE_KEY_CONTENT` in your `.env` file!
7. Note the **Key pair name** - you'll use this for `KEY_PAIR_NAME` in your `.env` file

**Option 2: Using AWS CLI**

```bash
# Create the key pair and save the private key
aws ec2 create-key-pair \
    --key-name airflow-training-key \
    --query 'KeyMaterial' \
    --output text > airflow-training-key.pem

# Set proper permissions (Linux/Mac only)
chmod 400 airflow-training-key.pem
```

**üìù Note:** The key pair name you choose will be used as the `KEY_PAIR_NAME` value in your `.env` file. The private key content (from the downloaded `.pem` file) will be used for `SSH_PRIVATE_KEY_CONTENT`.

### Step 1b: Get GitHub Personal Access Token

You need a GitHub Personal Access Token (PAT) to authenticate with GitHub's API and check CI status. Here's how to create one:

1. Log in to [GitHub](https://github.com/)
2. Click your profile picture (top right) ‚Üí **Settings**
3. Scroll down to **Developer settings** (left sidebar, at the bottom)
4. Click **Personal access tokens** ‚Üí **Tokens (classic)**
5. Click **"Generate new token"** ‚Üí **"Generate new token (classic)"**
6. Configure the token:
   - **Note:** Give it a descriptive name (e.g., "Airflow CI Checker")
   - **Expiration:** Choose an expiration date (or "No expiration" for development)
   - **Scopes:** Check the following boxes:
     - ‚úÖ `repo` (Full control of private repositories)
     - ‚úÖ `actions:read` (Read GitHub Actions data)
7. Click **"Generate token"** at the bottom
8. **‚ö†Ô∏è Important:** Copy the token immediately! GitHub will only show it once. It looks like: `ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`
9. Save this token - you'll use it for `GITHUB_PAT` in your `.env` file

**üîí Security Tip:** Never commit your token to Git. Always use environment variables or `.env` files (which are in `.gitignore`).

### Step 1c: Get Ngrok Auth Token (Optional)

Ngrok allows you to expose your local Airflow UI to the internet securely. This is useful if you want to access Airflow from your phone, share it with teammates, or receive webhooks. If you don't need remote access, you can skip this step.

1. Go to [ngrok.com](https://ngrok.com/) and sign up for a free account (or log in if you already have one)
2. After logging in, navigate to [Your Authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)
3. You'll see your authtoken displayed (it looks like: `2abc123def456ghi789jkl012mno345pqr678stu901vwx234yz`)
4. **Copy the authtoken** - you'll use it for `NGROK_AUTHTOKEN` in your `.env` file

**üí° What Ngrok does:**
- Creates a secure HTTPS tunnel from the internet to your local Airflow server
- Provides a public URL (e.g., `https://abc123.ngrok.io`) that forwards to `localhost:8080`
- Includes a web dashboard at `http://localhost:4040` to inspect requests and see the public URL

**üìù Note:** The free tier of Ngrok is perfect for development and testing. For production use, consider upgrading to a paid plan for custom domains and more features.

### Step 2: Create Environment File

Create a `.env` file in the project root with the following variables:

**üí° Tip:** You should have completed Step 1a (SSH key pair) and Step 1b (GitHub token) before filling in the values below.

```bash
# GitHub Configuration
GITHUB_REPO=your-username/your-repo-name
GITHUB_PAT=your_github_personal_access_token  # From Step 1b

# AWS Configuration
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=eu-west-3  # or your preferred region
KEY_PAIR_NAME=your-ec2-key-pair-name  # From Step 1a (e.g., "airflow-training-key")
AMI_ID=ami-00ac45f3035ff009e  # Ubuntu AMI (or your custom AMI)
SECURITY_GROUP_ID=sg-xxxxxxxxx  # Security group allowing SSH (port 22)
INSTANCE_TYPE=t3.small

# MLflow Configuration
MLFLOW_TRACKING_URI=http://your-mlflow-server:5000
MLFLOW_EXPERIMENT_NAME=california_housing

# SSH Private Key (for connecting to EC2)
# From Step 1a: Open the downloaded .pem file and paste its entire contents here
# Make sure to include the BEGIN and END lines!
SSH_PRIVATE_KEY_CONTENT="-----BEGIN RSA PRIVATE KEY-----
...
-----END RSA PRIVATE KEY-----"

# Airflow Configuration (optional)
AIRFLOW_UID=$(id -u)  # On Linux/Mac, this sets the user ID

# Ngrok Configuration (optional - for exposing Airflow UI publicly)
# From Step 1c: Paste your ngrok authtoken here (skip if you don't need remote access)
NGROK_AUTHTOKEN=your_ngrok_authtoken_here
```

**‚ö†Ô∏è Security Note:** Never commit the `.env` file! It's already in `.gitignore`.

**üì° About Ngrok:** If you completed Step 1c and added the `NGROK_AUTHTOKEN` to your `.env` file, Ngrok will automatically start when you run `docker-compose up`. You can view the ngrok dashboard at http://localhost:4040 to see the public URL and inspect requests. If you didn't set up Ngrok, the service simply won't start (which is fine for local-only access).

### Step 3: Set Airflow User ID (Linux/Mac)

```bash
export AIRFLOW_UID=$(id -u)
```

On Windows, you can skip this or set it manually in the `.env` file.

### Step 4: Initialize Airflow

This creates the database schema and an admin user:

```bash
docker-compose up airflow-init
```

Wait for the message: `"Airflow is ready!"`

### Step 5: Start Airflow Services

```bash
docker-compose up -d
```

This starts all services in the background:
- Airflow webserver (UI) at http://localhost:8080
- Airflow scheduler
- PostgreSQL database
- Redis broker
- Worker processes
- Ngrok (if `NGROK_AUTHTOKEN` is set in `.env`) - exposes Airflow UI publicly

### Step 6: Access the Airflow UI

**Local Access:**
1. Open your browser to **http://localhost:8080**
2. Login with:
   - **Username:** `airflow`
   - **Password:** `airflow`

**Remote Access (if Ngrok is configured):**
1. Visit **http://localhost:4040** to see the Ngrok web interface
2. Copy the public HTTPS URL (e.g., `https://abc123.ngrok.io`)
3. Use that URL to access Airflow from anywhere on the internet
4. The same login credentials apply (`airflow` / `airflow`)

### Step 7: Enable and Run the DAG

1. In the Airflow UI, find the DAG named `github_ec2_ml_training`
2. Toggle it from "Paused" (‚è∏Ô∏è) to "Unpaused" (‚ñ∂Ô∏è)
3. Click the "Play" button (‚ñ∂Ô∏è) to trigger a manual run, or wait for the schedule

### Step 8: Monitor Execution

- Click on the DAG name to see the graph view
- Click on individual tasks to see logs
- Green = success, Red = failure, Yellow = running

### Step 9: Stop Airflow (When Done)

```bash
docker-compose down
```

To also remove volumes (clears database):

```bash
docker-compose down -v
```

---

## Troubleshooting

### DAG Not Appearing in UI

- Check that `ml_training_pipeline.py` is in the `/dags` folder
- Look for import errors in the DAG processor logs: `docker-compose logs airflow-dag-processor`
- Ensure Python syntax is valid

### EC2 Instance Creation Fails

- Verify AWS credentials in `.env` are correct
- Check that your IAM user has `ec2:RunInstances` permission
- Verify the AMI ID exists in your region
- Ensure the security group allows SSH (port 22) from your IP

### SSH Connection Timeout

- Wait longer - EC2 instances can take 2-3 minutes to be SSH-ready
- Check security group rules allow SSH from your IP (or 0.0.0.0/0 for testing)
- Verify the SSH private key matches the key pair name in AWS

### Training Fails on EC2

- Check the task logs in Airflow UI for detailed error messages
- SSH into the instance manually and check `/var/log/user-data-debug.log`
- Verify MLflow tracking URI is accessible from the EC2 instance
- Ensure your GitHub repo contains a valid `MLproject` file for MLflow

---

## Next Steps

- **Customize the training:** Modify the `mlflow run` command in `run_training_via_ssh()` to pass parameters or use different entry points
- **Add monitoring:** Integrate with CloudWatch or Datadog to track instance metrics
- **Optimize costs:** Use Spot Instances for non-critical training jobs
- **Add notifications:** Send Slack/email alerts when training completes or fails
- **Scale up:** Modify the DAG to support parallel training runs on multiple instances

---

## Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Boto3 EC2 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html)
- [GitHub Actions API](https://docs.github.com/en/rest/actions/workflows)

---

**Happy Learning! üéì**

If you have questions or find issues, feel free to open an issue or contribute improvements!
